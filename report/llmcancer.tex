\documentclass[manuscript,screen,review,nonacm]{acmart}

\settopmatter{printacmref=false}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Using LLMs for Cancer Diagnosis from Clinical Notes}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Isaac Hands}
\email{isaac.hands@uky.edu}
\affiliation{%
  \institution{University of Kentucky}
  \country{USA}
}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Background and Introduction}
Cancer is a devastating illness, the second leading cause of death in the U.S \cite{cdc1}. One of the best ways to improve survival of this disease is to diagnose it as early as possible\cite{cruk1}. The way cancer is diagnosed in 90\% of cancer cases\cite{cdc2}is through a biopsy of tissue, which is then observed under a microscope by a pathologist, leading to a narrative description of the cancer cells in a pathology report. These reports consist of clinical narrative text dictated by a pathologist, stored in a patient's electronic medical record. The characterization of a cancer diagnosis occurs when a physician interprets a pathology report for communication to the patient and determines a treatment plan, along with data abstraction of the codes associated with the cancer diagnosis by specialized medical coders. This entire process of determining whether a patient has cancer and what type of cancer it is, relies on interpretation of narrative text. In fact, narrative unstructured text makes up about 80\% of all healthcare data\cite{Kong:2019tl}, making it difficult to analyze systematically. 

	Processing, analyzing, and interpreting unstructured text can be time consuming and cause delays in life-saving treatment for cancer patients, but may be an area where machine learning classifiers could provide efficiencies. In order to build classifiers, however, labeled training data needs to be readily available. When the data required is clinical text containing patient identifiers, it is often very difficult to access due to privacy concerns and lack of access to medical record systems. Moreover, clinical text uses a specialized vocabulary, which may prove problematic for natural language processing methods that have been developed around standard English vocabulary. In this report, we attempt to overcome these difficulties by utilizing a large language model (LLM) pre-trained on biomedical text, fine-tuned with clinical text documents that have a cancer diagnosis label. We will investigate both BERT\cite{devlin2018bert} and GPT\cite{radford2018improving}-based transformer\cite{vaswani2017attention} models, which have been shown to perform well on machine learning tasks in the biomedical realm\cite{wada2020pre}\cite{bbac409}.

\section{Materials and Methods}
We identified a dataset of clinical notes labeled with cancer diagnoses, along with BERT and GPT-type LLM models that could be fine tuned on standard hardware. The dataset we utilized, MIMIC-IV\cite{johnson2022mimic}, required a request for credentialed access for academic use from the authors, which was granted within one day. Even though the data was narrative text from actual hospital patients in a US, it was de-identified through an automated NER task\cite{johnson2020deidentification} before being made available, so privacy and security was not a concern. We also identified four transformer models, two BERT and two GPT, from the HuggingFace open source library, that were well characterized and able to be trained on a M1 Macbook Pro. Our programming for dataset preparation and model fine-tuning was done in Python 3.9 utilizing the PyTorch library\cite{paszke2019pytorch}. The source code used to process data and fine-tune the models is published in GitHub\cite{cs685}.

\subsection{Data Processing}

\subsection{Models Utilized}

\section{Results}

\section{Discussion}


\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\end{document}
\endinput