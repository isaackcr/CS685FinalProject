\documentclass[manuscript,screen,review,nonacm]{acmart}

\settopmatter{printacmref=false}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Using LLMs for Cancer Diagnosis from Clinical Notes}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Isaac Hands}
\email{isaac.hands@uky.edu}
\affiliation{%
  \institution{University of Kentucky}
  \country{USA}
}

\author{Anchit Bhattacharya}
\email{abh240@uky.edu}
\affiliation{%
  \institution{University of Kentucky}
  \country{USA}
}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Background and Introduction}
Cancer is a devastating illness, the second leading cause of death in the U.S \cite{cdc1}. One of the best ways to improve survival of this disease is to diagnose it as early as possible\cite{cruk1}. The way cancer is diagnosed in 90\% of cancer cases\cite{cdc2}is through a biopsy of tissue, which is then observed under a microscope by a pathologist, leading to a narrative description of the cancer cells in a pathology report. These reports consist of clinical narrative text dictated by a pathologist, stored in a patient's electronic medical record. The characterization of a cancer diagnosis occurs when a physician interprets a pathology report for communication to the patient and determines a treatment plan, along with data abstraction of the codes associated with the cancer diagnosis by specialized medical coders. This entire process of determining whether a patient has cancer and what type of cancer it is, relies on interpretation of narrative text. In fact, narrative unstructured text makes up about 80\% of all healthcare data\cite{Kong:2019tl}, making it difficult to analyze systematically. 

	Processing, analyzing, and interpreting unstructured text can be time consuming and cause delays in life-saving treatment for cancer patients, but may be an area where machine learning classifiers could provide efficiencies. In order to build classifiers, however, labeled training data needs to be readily available. When the data required is clinical text containing patient identifiers, it is often very difficult to access due to privacy concerns and lack of access to medical record systems. Moreover, clinical text uses a specialized vocabulary, which may prove problematic for natural language processing methods that have been developed around standard English vocabulary. In this report, we attempt to overcome these difficulties by utilizing a large language model (LLM) pre-trained on biomedical text, fine-tuned with clinical text documents that have a cancer diagnosis label. We will investigate both BERT\cite{devlin2018bert} and GPT\cite{radford2018improving}-based transformer\cite{vaswani2017attention} models, which have been shown to perform well on machine learning tasks in the biomedical realm\cite{wada2020pre}\cite{bbac409}.
	


\section{Materials and Methods}
We identified a dataset of clinical notes labeled with cancer diagnoses, along with BERT and GPT-type LLM models that could be fine tuned on standard hardware. The clinical note dataset we utilized, MIMIC-IV-Note\cite{johnson2022mimic}, required a request for credentialed access for academic use from the authors, which was granted within one day. Even though the data was narrative text from actual hospital patients in a US, it was de-identified through an automated NER task\cite{johnson2020deidentification} before being made available, so privacy and security was not a concern. We also identified four transformer models, two BERT and two GPT, from the HuggingFace open source library, that were well characterized and able to be trained on a M1 Macbook Pro. Our programming for dataset preparation and model fine-tuning was done in Python 3.9 utilizing the PyTorch library\cite{paszke2019pytorch}. The source code used to process data and fine-tune the models is published in GitHub\cite{cs685}.

\subsection{Data Description and Processing}
The MIMIC-IV-Note dataset consists of 331,794 discharge summaries and over 2 million free-text radiology reports representing clinical notes on more than 300,000 patients at the Beth Israel Deaconess Medical Center in Boston, MA. For this study, we focused on the discharge summaries due to our limited computational power available and the volume of the data. The data elements for each clinical note in the MIMIC-IV-Note dataset consisted of the actual narrative text of the note, three ID fields, and two timestamps\cite{dischnote}. In order to get labels for each note, the MIMIC-IV dataset\cite{johnson2023mimic} was used (note the difference between the names of the two datasets: MIMIC-IV-Note and MIMIC-IV). The MIMIC-IV dataset contains ICD-9 and ICD-10\cite{worldicd} labels for many of the clinical notes in the MIMIC-IV-Note dataset, linked by the three ID fields. Older notes had the older ICD-9 codes while newer notes used ICD-10. Each clinical note was labeled with multiple codes, assigned by medical billing coders when the patient was discharged from the hospital. 

The first step in processing the MIMIC-IV dataset was to filter out all labels that were not ICD-10 codes. We decided to focus exclusively on ICD-10 since it is a newer coding standard and to reduce the size of the fine-tuning dataset. Next, we filtered the MIMIC-IV-Note dataset to only include clinical notes that have ICD-10 labels and then joined the clinical notes with a list of their complete ICD-10 labels using the three ID fields. To simplify our machine learning task, we developed a binary cancer diagnosis classification scheme by labeling each clinical note as 'no cancer' (0) or 'cancer' (1) based on whether any of the ICD-10 labels for a note indicated a cancer code. In the ICD-10 coding vocabulary, all codes related to malignant cancers begin with the capital letter 'C', and no other ICD-10 codes have the capital letter 'C', so this binary labeling step was reduced to simply looking for the 'C' character among all of the ICD-10 codes. After the binary labels were attached to each note, we removed all data fields except the text and the label. 

The final step in processing the data set was to create randomized, split, balanced datasets, of varying sizes for training and development. Since most patients in the hospital system did not have a cancer diagnosis, we created balanced datasets where the number of notes labeled as cancer (1) matched the number of notes without cancer (0). We also wanted to have small, medium, and larger dataset sizes in order to quickly and progressively run the source code during development. We split the datasets into 70\% training and 30\% testing sizes, shuffled randomly in a balanced manner across both labels. The final balanced dataset sizes were: 100, 1000, 10000, and 22601.

\subsection{Models Utilized}
We selected four models from the HuggingFace open source model library for our experiments:
\begin{itemize}
  \item stanford-crfm/BioMedLM: a GPT based language model trained only on biomedical abstracts and papers from The Pile
  \item gpt2-xl: a pretrained language model based on GPT-2. The model is trained on the English language using causal language modeling.
  \item dmis-lab/biobert-base-cased-v1.2: a BERT based pretrained language model trained on PubMed abstracts and PMC full-text articles
  \item emilyalsentzer/Bio\_ClinicalBERT: initialized from BioBert and trained on all MIMIC-III notes
\end{itemize}
Two of the models tested were GPT and two were BERT-based, giving us a mix of encoder and decoder models from the transformer architecture. We chose these models because they were well cited in the literature and the largest models from HuggingFace that our computational platform could train and test within our time constraints. 


\section{Results}

We trained our four models on the 1000 document dataset described above, where we trained on 700 samples and tested on 300. We report the accuracy, precision, recall and F1 scores below on our test data after training the model for 10 epochs. 

\begin{center}
\begin{tabular}{||c c c c c c c ||} 
 \hline
 Model & Parameters & Style & Accuracy & Precision & Recall & F1 \\ [0.5ex] 
 \hline\hline
 stanford-crfm/BioMedLM & 2.7B & GPT & 0.833 & 0.816 & 0.888 & 0.850
 \\ 
 \hline
 gpt2-xl & 1.5B & GPT & 0.847 & 0.875 & 0.831 & 0.853 \\
 \hline
 dmis-lab/biobert-base-cased-v1.2 & 340M & BERT & 0.863 & 0.922 & 0.813 & \textbf{0.864} \\
 \hline
 emilyalsentzer/BioClinicalBERT & 340M & BERT & 0.850 & 0.857 & 0.863 & 0.860 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\section{Discussion}

Based on our results, we observed that the two BERT-style models outperformed both of the GPT-style models. The BERT-based dmis-lab/biobert-base-cased-v1.2 model produced the best F1 score, followed by Bio\_ClinicalBERT. Surprisingly, the results produced by the GPT-style BioMedLM model was similar to the GPT2-XL model, even though BioMedLM was pretrained on biomedical text, whereas GPT2-XL was trained on general English text. For future work, we would like to run similar experiments with these models on our largest dataset size of 22601, which was not possible with our limited computational resources and considering the total training time of the models. We would also like to explore better hyperparameter tuning to see if our results can be further improved. In terms of newer methods, we would like to explore zero shot learning with prompt engineering on the GPT-style models and compare to the BERT-style models. 


\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\end{document}
\endinput